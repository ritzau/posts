---
layout: post
title: Introduction to Concurrent Code
date: 2024-01-20 11:11:11 +0100
author: Tobias Ritzau
categories: draft guide cs concurrency introduction junior
---

Welcome to this introductory guide to concurrent programming! This guide is written to support any developer looking to get started with writing concurrent code.

We'll start off by exploring what concurrency is and why it's a powerful skill to master. Then, we'll delve into practical ways to implement concurrency using C++ and Kotlin. Next, we will examine common pitfalls and effective strategies to navigate them. By the end of this guide, you'll have all the essential knowledge needed to tackle concurrent programming tasks.

A quick heads-up: the examples provided are concise and optimized for readability across all devices. The level of brevity used here is greater than what I'd recommend for production code. Always strive to keep production code readable. All code presented in the guide is also available [online](https://github.com/ritzau/posts), complete with instructions for building and running it. The code in the repository is better formatted but is written primarily to explain the examples, not for production.

## Introducing concurrency

When we start learning to code, we begin with _sequential_ programs, where operations are executed one after another in a single flow. Today, let's step into the realm of _concurrent code_ — a major shift that enables a program to manage multiple sequences of operations simultaneously.

Most modern computers run multiple programs concurrently. Starting a program creates a _process_, which represents the executing program. A process may spawn other processes during its lifetime. For instance, an application launcher creates processes to launch apps. While this is concurrent programming, this guide focuses on concurrency within a single process, which is where you'll likely encounter it most. However, many of the techniques discussed are also relevant to managing concurrent processes.

Concurrency within a process fundamentally relies on _threads_. Think of threads as distinct paths of execution in your code. A program starts with a single thread, the _main thread_, which invokes the main function. From there, you can create additional threads as needed, akin to how a process can create new processes. Why start more threads? Here are a few practical scenarios:

- Fetching data in one thread while keeping the user interface responsive in another.
- Calculating a game's next move in one thread while rendering the current scene in another.
- Handling multiple client requests in a web server, each managed by a separate thread.

Modern hardware architecture necessitates efficient concurrent code. Processors now often feature multiple cores, each acting like a separate processor. A multi-core processor can run several threads in parallel, but this only happens if there are multiple ready-to-run threads. Effectively, you must divide the workload across the cores.

It's a common misconception that concurrency benefits only multi-core processors. Single-core processors also benefit from concurrency. For example, a core waiting for network data can switch to other tasks instead of idling. This mirrors everyday multitasking, like cooking dinner while waiting for the next episode of your favorite TV show.

Remember, concurrency doesn't universally speed up all processes. Some processes, like pregnancy, are inherently sequential and can't be expedited, regardless of the number of people involved.

Concurrency offers powerful capabilities but adds complexity. Incorrectly implemented, concurrent code can lead to elusive bugs that are challenging to fix. So why delve into this intricate domain? Let's dive deeper to unravel the mysteries of concurrency.

## Benefits of concurrency

Embracing concurrency in programming unlocks several key advantages. These benefits not only enhance the performance of applications but also contribute to more efficient and maintainable code. Let's explore the three primary benefits:

### 1. Efficiency

The evolution of processors has shifted from increasing clock speeds to adding more cores. This change underscores the growing importance of concurrency in modern computing. Think of a busy restaurant kitchen: having several chefs (cores) working on different dishes (threads) simultaneously maximizes kitchen throughput. However, the addition of more chefs is only effective if there are enough tasks to keep them busy. Similarly, to fully utilize multi-core processors' capabilities, applications need to run multiple threads, allowing each core to contribute to the overall processing power.

### 2. Responsiveness

System responsiveness is crucial in various contexts. For example, in a web server scenario, handling HTTP requests sequentially is akin to a single waiter managing and cooking all orders in a restaurant, leading to delays and unhappy guests. By contrast, applying concurrency to a web server can be compared to a team of waiters swiftly taking orders and passing them to the kitchen, significantly improving response times.

This principle applies to UI applications as well, where a main thread dedicated to rendering should not be burdened with time-consuming tasks. Offloading such tasks to separate threads or processes keeps the main thread available for rendering and user interactions, similar to waiters efficiently managing orders while the kitchen handles the cooking.

### 3. Separation of concerns

Concurrency also facilitates the division of a program into logical components, streamlining both development and maintenance. Back to the restaurant analogy: it’s more efficient to provide a chef with three separate recipes for different courses than one convoluted set of instructions for all three courses. This separation allows the chef to manage tasks effectively, switching between them based on progress and priority. Similarly, when programming tasks are divided into distinct concurrent components, they become less complex and more adaptable, enhancing the overall code structure and flexibility.

## Writing concurrent code

Concurrent programming offers a wealth of opportunities to enhance both the performance and responsiveness of applications. Let's delve into some commonly used methods for writing concurrent code, showcasing examples in C++ and Kotlin.

### Threads

Threads form the backbone of concurrency, each representing a unique path of execution within a process. Initially viewed as lightweight alternatives to running multiple processes, threads are now acknowledged for their inherent complexity and resource demands. We'll start with an exploration of threads before progressing to more advanced and efficient concepts.

When you create a thread, you provide it with a function to execute. The life cycle of the thread is closely tied to this function - when the function's execution ends, so does the thread.

#### Example: C++ Thread

Creating a new thread in C++ is relatively simple. You define a function or a lambda expression and pass it to the `std::thread` constructor. This action causes the thread to start executing concurrently with the main thread. It's important to note that you must invoke `std::join` on the thread before its instance is destroyed (which generally happens when it goes out of scope). Neglecting to do so results in the program terminating unexpectedly. To mitigate this risk, C++20 introduced `std::jthread`, which automatically handles the joining of the thread upon its destruction, provided it hasn't been joined already.

{% include replit.md href="" %}

```cpp
// Define a lambda function
// to say hello
auto sayHello = []{
    cout << "Hello, "
         << this_thread::get_id()
         << endl;
};

// Create and start a new thread
thread t(sayHello);

// Call the function in the
// main thread
sayHello();

// Wait for the new thread
// to finish
t.join();
```

#### Example: Kotlin Thread

Kotlin supports multiple runtimes. In this guide, "Kotlin" refers to Kotlin running on the JVM. To create a thread in Kotlin, you define an instance of the `Runnable` interface, either by implementing it or using a lambda expression. This instance is then passed to the `Thread` constructor. Unlike some languages, the thread in Kotlin does not start with the constructor; instead, you must start it manually using the `start` method. After starting, you can join the thread with the `join` method. The program waits for the thread to complete before exiting, unless you have explicitly specified otherwise.

{% include replit.md href="" %}

```kotlin
// Define a lambda function to
// say hello
val sayHello = {
  val tid = Thread.currentThread().id
  println("Hello, $tid")
};

// Create and start a new thread
val t = Thread(sayHello)
t.start()

// Call the function on the
// main thread
sayHello();

// Wait for the new thread
// to finish
t.join();
```

These examples illustrate the basic usage patterns of threads in both C++ and Kotlin. As you delve deeper into concurrent programming, you'll encounter more advanced scenarios and techniques for managing threads.

Threading in both C++ and Kotlin enables the execution of multiple operation sequences concurrently, a crucial concept for leveraging the power of modern multi-core processors and optimizing application performance.

### Task queues and executors

Threads offer a lightweight alternative to processes but still come with scalability limitations. A more effective strategy is to split code into manageable chunks for execution via task queues. These queues utilize a thread pool, which can be configured with either a fixed number of threads or a dynamic size. A notable variant is the serial task queue, using just one thread, allowing submissions from multiple threads to be executed sequentially on the same thread.

Different programming languages have unique terminologies and implementations for task queues. In the JVM, these are referred to as _Executors_. Swift calls them _Dispatch Queues_, while Kotlin introduces _Dispatchers_ and a novel approach to _coroutines_. We'll touch on these briefly here, and delve into more detail in a subsequent post.

#### Example: C++ async call

In C++, the standard library considers the thread pool as an implementation detail, providing limited control over its execution process. With the advent of C++11, the language introduced a mechanism for invoking functions asynchronously. When using the `async` function, you have the option to specify an execution policy. By opting for `std::launch::async`, the function is executed on a separate thread. The default setting permits the runtime to select the most appropriate policy.

{% include replit.md href="" %}

```cpp
// Define a lambda function to
// say hello
auto sayHello = [] {
  cout << "Hello, "
    << this_thread::get_id()
    << endl;
};

// Launch the function on a
// separate thread
auto future =
  async(launch::async, sayHello);

// Call the function on the
// main thread
sayHello();

// Wait for the async function
// to finish
future.get();
```

#### Example: Kotlin coroutine

In Kotlin, you have the option to use the Executor framework, but the language also introduces an advancement with the concept of _coroutines_. A coroutine, in general, is a function that can pause and resume its execution, which allows it to yield multiple values. This is in contrast to traditional functions that return only a single value. Kotlin's coroutines further extend this concept with features like concurrent execution.

Kotlin coroutines operate within a specific coroutine context. The default context provides a single-threaded task queue, determined by the context's dispatcher. For scenarios requiring multi-threading, `Dispatchers.DEFAULT` can be employed. This dispatcher is designed to provide as many threads as there are processor cores, enabling efficient parallel execution.

{% include replit.md href="" %}

```kotlin
// Define a lambda function to
// say hello
let sayHello = {
  val tid =
    Thread.currentThread().id
  println("Hello, from $tid")
}

// Create and start a coroutine and
// call the function in it
launch(Dispatchers.DEFAULT) {
    sayHello()
}

// Call the function on the
// main thread
sayHello()

// The runtime implicitly waits
// for all coroutines launched
// in the current scope to
// run to completion
```

These examples from C++ and Kotlin show how modern programming languages manage and optimize task execution in concurrent applications. Each language offers unique tools and abstractions, from executors to asynchronous functions and coroutines, enhancing scalability and performance. Understanding the specifics of each language is less important than grasping the overall concepts, which remain consistent across different languages.

## Pitfalls

Understanding the nuances of concurrency and its implementation is only the beginning. Awareness of potential issues arising from concurrent execution is crucial. These problems can be elusive, manifesting as rare "once in a million" bugs, and are notoriously difficult to identify and fix. Code that is safe to execute concurrently by multiple threads is said to be _thread safe_.

### Data races and race conditions

Two major categories of issues commonly arise in concurrent code:

- _Race Conditions_: These occur when the timing of uncontrollable events affects the outcome of a computation. While sometimes harmless or intentional, they often lead to unexpected and erroneous results. Consider a restaurant where the sequence of dish preparation varies. Normally, this is fine, but if desserts are served before the main courses, it indicates a problem.

- _Data Races_: These happen when two or more threads access the same resource without proper synchronization, and at least one thread modifies it. Often involving shared variables, data races can cause threads to work with outdated or incorrect states of the resource, leading to complications. For instance, imagine a chef mistakenly adding salt instead of sugar to a dessert due to confusion in a busy kitchen.

_Data races_ can also lead to _tearing_, where only part of a resource is updated, potentially resulting in corrupted data and unpredictable behavior. For example, if one thread partially updates a 64-bit value while another reads it, this could cause program crashes or other serious issues on a 32 bit architecture.

#### Example: C++ data race

Here is an example demonstrating a data race, where two threads modify a `counter` variable – one incrementing and the other decrementing. Due to the nature of concurrent execution, the final value of `counter` might not be as expected.

{% include replit.md href="" %}

```cpp
const auto n = 1'000'000;

// Shared counter
auto counter = 0;

// The '&' allows the lambda to
// reference any in-scope variable
auto increase = [&counter] {
  for (auto i = 0; i < n; ++i) {
    // Update shared variable
    ++counter;
}
};
auto decrease = [&counter] {
  for (auto i = 0; i < n; ++i) {
    // Update shared variable
    --counter;
}
};

auto t1 = thread(increase);
auto t2 = thread(decrease);

t1.join();
t2.join();

// Result is undefined due to
// the data race
cout << "Counter: "
  << counter
  << endl;
```

The challenges posed by data races in programming boil down to three aspects:

1. **Sequentiality**: Operations within a thread are sequential, but the ordering of operations across different threads is unknown.
2. **Visibility**: The various caching levels between executing instructions and main memory make it hard to predict when an instruction's result becomes visible to another thread.
3. **Optimization**: To optimize performance, compilers and processors often reorder instructions, further complicating the matter.

To manage these challenges, it's necessary to establish rules determining which operations logically **happen before** others.

## Thread-safe code

Now let's explore how to structure concurrent code to prevent data races and manage race conditions. Key to this is isolating code that accesses a shared state and understanding the _happens-before_ relationship.

By keeping the shared state and the code that accesses it isolated, you significantly simplify understanding the code, even when returning to it later. This isolation makes it easier to modify the code and verify that it remains thread-safe.

Compilers and processors adhere to specific rules to ensure which operations **happen before** others, regardless of which thread they execute on. These rules constitute the memory consistency model, defined at the language/runtime level. Therefore, these rules vary between languages/runtimes, and it's essential to understand the rules for the setup you are using. Here's a subset of these rules that hold true for both Kotlin and C++, ensuring thread safety – the ability to safely execute code concurrently by multiple threads:

1. **In-Thread Sequence**: Each operation in a thread follows the one before it, establishing a clear order within the thread itself.
2. **Thread Start**: Operations executed in a thread before starting a new thread "happen before" all operations in the new thread. This is also applicable to Kotlin coroutines and tasks in task queues.
3. **Thread Join**: Operations executed in a thread "happen before" those executed after joining the thread.

A _future_ is a powerful concept that enables asynchronous access to the outcome of a computation and is a thread-safe way to pass values in concurrent code. A future represents a value that may not yet be available but will be in the future. In some contexts, these are called _Promises_. Futures and promises can be completed either with a value or an error.

#### Example: Thread-safe Kotlin

Consider this example using `async()` in Kotlin and C++, where the result of concurrent code execution is available as a future:

{% include replit.md href="" %}

```kotlin
// Operations here are guaranteed
// to execute before those in
// the coroutine
val foo = 42
val future = async {
  // Safe access to foo, but
  // no ordering guarantees
  // for modifications by other
  // threads after this
  // coroutine starts
  foo + 1337
}
// Exercise caution until the
// future's result is awaited
val result = future.await()
```

For concurrency safety, making data accessed within concurrent code immutable is recommended, as it eliminates race condition risks. If immutability isn't feasible, copying or moving the data into the thread ensures exclusive access.

#### Example: Thread-safe C++

Here's an example using futures and local data copies to avoid data races:

{% include replit.md href="" %}

```cpp
const auto n = 1'000'000;
auto counter = 0;

// Copying data into the
// lambda ensures no
// race condition
auto f1 =
  [local_counter = counter]() mutable {
    for (auto i = 0; i < n; ++i) {
      // Update local copy
      ++local_counter;
    }
    return local_counter;
};

auto f2 =
  [local_counter = counter]() mutable {
    for (auto i = 0; i < n; ++i) {
      // Update another local copy
      --local_counter;
    }
    return local_counter;
};

// Concurrently execute
// without risk of
// data race
auto t1 = async(f1);
auto t2 = async(f2);

// Await results and combine
auto result = t1.get() + t2.get();

// Consistent and predictable
// outcome
cout << "Counter: "
     << result
     << endl;
```

## Key Takeaways

- **Concurrent code** helps you improve **separation of concerns**, **efficiency**, and **responsiveness**.
- **Threads** are the foundational abstraction for executing concurrent code.
- **Task queues**, aka. **Executors**, increase the efficiency and control of concurrent code.
- Either ensure that your code is only executed by a single thread (e.g., on a serial executor), or that it is **thread-safe**.
- A **race condition** makes the result of your code dependent on external events out of your control. This may be harmless and even intentional, but it often results in bugs.
- A **data race** happens when multiple threads access the same resource without proper synchronization, and at least one thread modifies the resource. Data races cause issues with **sequentiality**, **visibility**, and **optimizations**.
- The **Happens-before** relationship and the **Memory consistency model** provide guarantees to help you write thread-safe concurrent code.
  - Code before a thread start happens before the code in the thread.
  - Code in the thread happens before code after the join.
- A **future** represents a thread-safe future outcome of a computation.
- Most importantly, to make it easier to reason about your concurrent code, you should **isolate code that concurrently accesses a shared state**.

Equipped with a thorough understanding of concurrency's principles and advanced techniques, you're now better prepared to leverage its power in your programming endeavors.

## Next Steps

As you apply these principles to your projects, more complex scenarios and challenges may arise. Our next guide will delve into synchronization mechanisms and explore their potential pitfalls. Stay tuned for a deeper dive into the advanced realms of concurrency mastery!
