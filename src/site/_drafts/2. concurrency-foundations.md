---
layout: post
title: "Concurrency: Synchronization"
date: 2024-01-27 14:24:26 +0100
author: Tobias Ritzau
categories: wip post blog cs concurrency synchronization deadlocks
---

Welcome to the next step in our journey through concurrent programming. Following up on the [Introduction to Concurrency]() post, which laid the groundwork by discussing threads, task queues, thread safety, and the happens-before principle, we're now set to dive deeper.

This guide will focus on scenarios where fine-grained control is essential. In the introduction, we learned that a data race might occur if multiple threads access the same resource, with at least one thread modifying it, all without proper synchronization. We addressed this by sharing immutable data or allowing each thread to work on its own data copies. This time we will make our code thread-safe by addressing synchronization.

Recall the happens-before rules:

- Operations before a thread's initiation are visible within the thread.
- Actions within a thread occur before the thread has been successfully joined.

We also touched on futures, a means for asynchronously and thread-safely passing data between threads.

Now, we shift our focus to synchronizing threads and their access to shared data. This approach enables us to read and modify data from different threads but introduces the risk of deadlocks. I'll explain what deadlocks are, how they occur, and strategies to avoid them.

A quick heads-up: the examples are concise and optimized for readability across all devices. The level of brevity here is greater than what I'd recommend for production code. Strive for readability in production code. All code in this guide is also available [online](https://github.com/ritzau/posts), with instructions for building and running it. The repository code is better formatted but primarily serves to explain the examples, not for production use.

---

## Below is a copy of the attic...

## Introducing Concurrency

A non-concurrent program executes operations in a single sequence, while a _concurrent_ program executes multiple sequences of operations concurrently. Sequences are typically started and stopped dynamically during the execution of the program. Example use include:

- Fetching data while rendering a user interface.
- Computing the next state of a game while rendering the current state.
- A web server that processes the requests of multiple clients at the same time.
- A GPU that runs shaders to compute color the pixels of a scene.

There are three main reasons to use concurrency:

- Efficient use of the hardware
- To simplify the division of logical components of a program
- To avoid blocking the UI thread.

When it comes to efficiency, the clock frequency of processors have not increased much the last ~20 years, while the number of processor cores is steadily increasing. To put the cores to use, the code needs to be concurrent. For a physical world comparison, consider a restaurant. A restaurant can be operated by a single person, but the orders will be taken faster and the food will be delivered faster with a staff of waiters; likewise the food will be cooked faster with a staff of chefs. Modern processors, even in mobile phones have multiple cores.

An example of the logical split is to implement a producer of data and the consumer of the data as two different components without the need to handle the scheduling of these components. In the restaurant case, it is far easier for a chef to follow separate recepies for the courses they cook, than to try combine all recepies into one set of instructions. The chef will schedule their work by switching between the recepies, and all the chefs works mostly independenly on the courses they prepare. Much like CPU cores execute sequences of operations.

Mobile and Web all use a main thread which is responsible for rendering the UI. That thread must not be blocked, or you will see glitches in responsiveness, or even have your program crash. Back to the restaurant, if the waiter and the chef is the same person, then guests would have a hard time getting response from the waiter while they are working in the kitchen. Instead, the waiters hand off to the chefs and can get back to the guests of the restaurant while the chefs continue their work.

It's worth noting that concurrency benefits aren't limited to multi-core processors. Even on single-core systems, cores often idle while waiting for I/O operations, like reading from storage, network communication, or awaiting user input. In these scenarios, concurrency enables the execution of other tasks during these idle periods, optimizing resource utilization. Just like a single chef can prepare several dishes at the same time.

It’s also worth noting that concurrency is not a tool that can be used to increase performance in all situations, as in the case of pregnancy: a woman can't get a baby in a month with the help of 9 men. Some things are inherently sequential, more cores will not speed those operations up, algorithms and better use of a single core is the way to optimize such code.

## Writing Concurrent Code

There are countless ways to write concurrent code. This section provides some examples of common approaches uses in popular runtimes.

### Threads

```
Add note on safely passing data in
```

A straight forward approach available in most runtimes is the direct use of threads. A _thread_ simply represent a separate sequence of execution whithin a process. Threads within a processes share most things except CPU registers and the stack. Switching between threads happens by pausing the thread, swap the registers and the stack, and then resume the new thread.

When introduced, threads were considered very light weight, but the view on that has changed. The typical way to start a thread is to specify a function that should be invoked by the thread. When the function exits, the thread has run to completion. Note that a single thread, often called the 'main thread', is always available and executes the main function of your program. In the example below a new thread is created and launched concurrently with the main thread. Both threads then output their thread ID, before the main thread waits for the created thread to come to an end, at which point the threads can _join_.

{% include replit.md title="Example: C++ threads" href="concurrency-foundations-threads-cpp#main.cpp" %}

```cpp
auto sayHello = []
    { cout << "Hello, " << this_thread::get_id() << endl; };

auto t = thread(sayHello);
sayHello();

t.join();
```

### Executors

```
Add note on safely passing data in and out. Note futures.
```

Threads are cheaper than processes but they don't scale well. Instead of mapping sequences of instructions to separate threads, the code can be split into executables that can be executed on existing threads in a pool. The abstraction is often called something along the lines of _Executors_ or _Thread queues_. A piece of code can be submitted or enqueued to be executed by an executor.

There are to main scenarios: either the code is split into chunks that doesn't block, meaning that the threads of the pool will never block if there is work to do; or a thread pool is used to limit the concurrent use of resources such as network connections. A special case occurs when a single thread is made available to a queue, this is called a _serial queue_ and is a powerful tool used to battle concurrency issues as described below. In advanced scenarios you can allow executors for different scenarios to share an underlying thread pool.

An example of that is presented in the Java example below.

{% include replit.md title="Example: Java executors" href="" %}

```java
Runnable sayHello = () ->
	System.out.println("Hello, " + Thread.currentThread().getId());

var executor = Executors.newFixedThreadPool(10);
var future = executor.submit(sayHello);
sayHello.run();

future.get();
executor.shutdown();
```

C++ introduced an abstraction for this in C++11 where a function can be asked to execute asynchronously. The execution policy determines how the code is executed, passing `std::launch::async` specifies that the function will be executed on a different thread.

{% include replit.md title="Example: C++ <kbd>async()</kbd>" href="" %}

```cpp
auto sayHello = []
    { cout << "Hello, " << this_thread::get_id() << endl; };

auto future = async(launch::async, sayHello);
sayHello();

future.get();
```

### Coroutines

Some languages take this a step further and splits the code up into non-blocking chunks automagically. It's really a best effort since a developer still can block threads, but if the developer sticks to asynchrnous operations, threads never need to block. In some case the thread pools are also abstracted away. The concept of _coroutines_ in Kotlin is an example of this. Coroutines have different interpretations in different languages, at the core a coroutine is a function that can yield control while executing, and resume it from where it left off.

Kotlin coroutines are designed to yield control instead of blocking. In theory, this makes it easy to write code that never blocks a thread and thus if you have a pool with the same number of threads as the number of cores, then you can use the CPU very efficiently. Note that Kotlin coroutines execute in a context. The context controls the execution and the default is a serial queue. By specifying the default dispatcher when launching a coroutine, you ensure that it can execute concurrently with other coroutines. In most cases you don't specify the dispatcher like this though. You have a context and only modify that when needed.

{% include replit.md title="Example: Kotlin coroutines" href="" %}

```kotlin
launch(Dispatchers.DEFAULT) {
	println("Hello, from ${Thread.currentThread.id}")
}

println("Hello, from ${Thread.currentThread.id}")
```

### Inter Process Concurrency

It should be noted that concurrency doesn’t only exist within a process. The code may very well run in different processes, or even on different hosts. Even if the best practices presented here still apply, we stick to talk about concurrency within a process in this post.

## Pitfalls

Now we now what concurrency is and have any idea of how to write concurrent code, but before jumping in you have to be aware on issues that can be caused by concurrent code. These issues are can be very hard to identify, and are often intermittent happning "once in a million" bugs. Therefore it is crucial to understand what to watch out for when writing and reviewing code. Always consider if the code you write and review can be run concurrently.

### Data Races and Race Conditions

A _race condition_ occurs when the timing of events that are out of the control of the developer affects the outcome of a computation. That typically means that you will can see different results when you run the code, and that may be all fine, but it may also be a bug. In the restaurant a race condition may cause the order of the completion of main dishes to vary even if the same dishes are prepared. That is fine, but if the desserts are delivered before the main dishes, you have a problem if your guests are not extremely flexible ;)

A _data race_ may happen when two or more threads access the same resource and at least one thread is modifying it. The resource is typically a variable. Data races can result in some threads not seeing the current state of the resource which can cause a bug itself, but it can also use the invalid state to update the state, and that quickly goes south. Back in the restaurant, a race condition can cause a chef to add curd to the lasagna.

Consider two threads, A and B, that both increment the value of an integer. Each thread reads the value, increments it, and stores it back. Due to scheduling we can have different outcome as can be seen below.

| Thread A  | ThreadB   | Variable |
| --------- | --------- | -------- |
| `read 0`  |           | 0        |
| `inc 1`   |           | 0        |
|           | `read 0`  | 0        |
|           | `inc 1`   | 0        |
|           | `write 1` | 1        |
| `write 1` |           | 1        |

Or with some "luck":

| Thread A  | Thread B  | Variable |
| --------- | --------- | -------- |
| `read 0`  |           | 0        |
| `inc 1`   |           | 0        |
| `write 1` |           | 1        |
|           | `read 1`  | 1        |
|           | `inc 2`   | 1        |
|           | `write 2` | 2        |

> Example of two threads incrementing an int

Under some conditions, a data race can also lead to _tearing_, meaning that the resource is only partially updated. A partially updated resource typically leads to corrupt data and as such, undefined behavior. It can for example be that only 32-bits of a 64-bit value is updated when a thread observes it. If that represents the funds of a bank account, the program will continue to run, but if it is an index that index out of bounds of an array, or if the value represents a pointer, then the program may crash. A crash is often better because the bug has been detected. The invalid bank account can go unnoticed which is most likely worse.

The following example demonstrates a data race. Depending on your setup, you may need to adjust the number of iterations to get the (un)desired outcome. Here we have two threads that read and update the `counter` variable. Given the nature of modern hardware architectures you should see that `counter` is not equal to `2 * n` given a large enough n.

{% include replit.md title="Example: C++ data race" href="" %}

```cpp
const auto n = 1'000'000;
auto counter = 0;

auto f = [] { for (auto i = 0; i < n; ++i) ++counter; };
auto t1 = thread(f);
auto t2 = thread(f);

t1.join();
t2.join();

cout << "Counter: " << counter << endl;
```

Interestingly enough, you can't write this example in safe Rust. Rust requires you to be expliit about ownership and strict about what you can accesss from a thread. If you want to pass data to the thread, you need to either move it in, make a copy, or it needs to support multithreaded access. If these conditions are not fulfilled the program will not compile. JavaScript workers gives you similar guarantees.

## Avoiding The Racces

In the words of the great Mr. Miyagi, the best defense is not to be there. In many cases, it is possibly to avoid data races by breaking the invariant of a data race. A data race requires multiple threads to access the same resource and at least one needs to modify the resource. Thus we can avoid the race by:

- Accessing the resource from a single thread
- Prevent modification

You can ensure that only one thread can access a resource by:

- Using a serial queue to access the resource
- By creating a copy for each thread
- By moving the resource to the thread that needs access (and by that not making it possible for any other thread to access it.)

### Single Threaded Access

Using a serial queue, or a variant there of, is common when it comes to UI frameworks. For example both Android, iOS, and Web only allow access to the UI hierarchy from the main thread. To make it efficient, you need to limit the access to the shared data and perform as much of the computation without it and then, typically in the end but periodically also works, update the shared data. An example would be to load data from a file on a separate thread, and then update the UI with this data on the UI thread.

{% include replit.md title="Example: Kotlin serial queue" href="" %}

```kotlin
x
```

The efficiency of using this teqnique all depends on how much of the computation that happens on the single thread. Most of all, this teqnique is simple to prove correct, and if efficiency is not the goal of the use of concurrency and the computation on the single thread is kept short, this is a good solution.

### Copy Data

Copying the data may be enough for some cases, but you often want to communicate some result back, and then have some thread responsible of combining the outcomes of the other threads, possibly hierarchically. In the data race example above, each thread can increment their own counter and then communicate the result back, and the main thread would then sum up the results giving the expected answer. This can easily be achieved using futures. To make it more interesting, we will not simply increment a counter, but instead sum up the elements of an array. The `[=]` notation specifies that data is copied into the threads.

> The data in this example is read-only, and the example is not good. Replace.

{% include replit.md title="Example: C++ copy data" href="" %}

```cpp
int future_sum(const vector<int>& numbers) {
  auto start = numbers.cbegin();
  auto split = numbers.cbegin() + numbers.size() / 2;
  auto end = numbers.cend();

  auto f1 = async([=] { return reduce(start, split); });
  auto f2 = async([=] { return reduce(split, end); });

  return f1.get() + f2.get();
}
```

### Immutable data

Immutability is a powerful tool in runtimes that lacks value semantics. Value semantics means that when you store a value, you will get a copy, the same holds when you pass values to a function. If you use reference semantics you will instead pass a reference to the same data. This is called _aliasing_, one value may have multiple aliases. Though this is not a problem if no one modifies the value. You can often avoid concurrency issues by making data that should not change immutable.

> Example: Possibly Kotlin passing a string

### Moving Data

Moving data is common in C++, Rust, and with Web Workers. This does not allow concurrent access, but rather ensures that only one thread **can** access a resource at any point in time.

{% include replit.md title="Example: Rust immutable data" href="" %}

```rust
let counter = Arc::new(0);

let t = thread::spawn({
    let counter_alias = counter.clone();
    move || { *counter_alias + 42 }
});

let result = t.join().unwrap();
println!("counter {} from the main thread!", result);
```

In this example we create an atomically reference counted (Arc) integer initialized to 0. By default, data referenced by an Arc is immutable. That means that when a new thread is spawned we can clone the reference, still refering to the same data, and safely access it from within the thread. The `move` modifier to the lambda means that the `counter_alias` referenced is moved it into the thread, and thus can no longer be accessed outside of it. More on that below. Also note that `||` is not a logical or operator, it is an empty list of arguments to the closure.

### Thread Safe Types

Thread safe data structures and atomics is a topic for another note. For now, let's just say that an atomic type allows you to read and update variables atomically and it ensures that no data race can occur. The data race in the counter example can be fixed by simply making the counter atomic.

```cpp
const auto n = 1'000'000;
auto counter = atomic_int(0);

auto f = [] { for (auto i = 0; i < n; ++i) ++counter; };
auto t1 = thread(f);
auto t2 = thread(f);

t1.join();
t2.join();

cout << "Counter: " << counter << endl;
```

When using thread safe types, we delegate the responsibility of thread safety to another implementation. As long as we trust that implementation we can avoid the complexity. Atomics are typically implemented in hardware and quite cheap, though they only cover primitive types. In addition to atomics, the JVM standard library has a number of powerful thread safe data structures such as `ConcurrentHashMap` which can be very useful, but note that while thread safety makes it possible to access data from multiple threads, it doesn't allow you to atomically update multiple variables. If that is what you need, you will need to synchronize access and if you do you no longer need the thread safe type. More on that in the next section.

## Key Takeaways

- **Leveraging Concurrency for Efficiency**: Concurrency is a powerful tool to enhance the performance of your code, particularly on modern multi-core processors, allowing for parallel task execution.
- **Essential for UI Responsiveness**: In applications like mobile apps, concurrency is vital to prevent blocking the UI thread, ensuring a smooth and responsive user interface. (FIXME: also for a service handling requests)
- **Diverse Techniques for Implementation**: Implementing concurrency can be achieved through various methods, including threads, executors/queues, and coroutines, each suitable for different scenarios and needs.
- **Navigating Race Conditions**: One of the primary challenges in concurrent programming is avoiding race conditions. These occur when multiple threads access the same data simultaneously, with at least one modifying it, potentially leading to inconsistent results.
- **Strategies to Prevent Race Conditions**:
  - Ensure exclusive data access by a single thread.
  - Use thread-safe data structures and atomic operations.
  - Employ data copying or handover strategies for thread exclusivity.
  - Move data
  - Embrace immutability where feasible.

These insights offer a comprehensive overview of concurrency, its benefits, challenges, and strategies for safe and efficient implementation. Equipped with this knowledge, you're prepared to tackle concurrency in your coding projects with confidence.

## Next Steps

As you begin to integrate these principles into your projects, you'll likely encounter more complex scenarios and challenges. In the next section, "Advanced Concurrency Concepts," we'll dive into synchronization mechanisms and their pitfalls. Stay tuned as we unlock the next level of concurrency mastery!

## Terminology

- Concurrency
- Data race
- Race condtition
- Thread
